{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Layer, GRU, Bidirectional, Dense, Input, Reshape, GlobalAveragePooling1D\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...</td>\n",
       "      <td>other</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>বাংলাদেশের পরে ভারতের সাপর্ট ই করি ?</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>সৌম্যকে বাদ দেওয়া হোক</td>\n",
       "      <td>batting</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>প্রথমটি হচ্ছে, কোচ অত:পর সাকিব,সাকিব আর সাকিবর...</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Category  Polarity\n",
       "0  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...    other  positive\n",
       "1  জয় বাংলা কাপ! তাও আবার স্বাধীনতার মাস মার্চে। ...     team  positive\n",
       "2               বাংলাদেশের পরে ভারতের সাপর্ট ই করি ?     team  positive\n",
       "3                              সৌম্যকে বাদ দেওয়া হোক  batting  negative\n",
       "4  প্রথমটি হচ্ছে, কোচ অত:পর সাকিব,সাকিব আর সাকিবর...     team  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Cricket - Sheet1.csv\")\n",
    "df = df[['Text', 'Category', 'Polarity']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...</td>\n",
       "      <td>other</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>বাংলাদেশের ভারতের সাপর্ট</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>সৌম্যকে বাদ</td>\n",
       "      <td>batting</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>প্রথমটি কোচ অতপর সাকিবসাকিব সাকিবরে দলে</td>\n",
       "      <td>team</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Category  Polarity\n",
       "0  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...    other  positive\n",
       "1  জয় বাংলা কাপ স্বাধীনতার মাস মার্চে মাথা চমৎকার...     team  positive\n",
       "2                           বাংলাদেশের ভারতের সাপর্ট     team  positive\n",
       "3                                        সৌম্যকে বাদ  batting  negative\n",
       "4            প্রথমটি কোচ অতপর সাকিবসাকিব সাকিবরে দলে     team  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize Bengali stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)  # Keep only Bengali characters\n",
    "    text = re.sub(r'\\d+', '', text)                 # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Remove extra spaces\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution after upsampling:\n",
      "Category\n",
      "bowling            2799\n",
      "batting            2226\n",
      "team               2094\n",
      "other              1913\n",
      "team management    1468\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Polarity distribution after upsampling:\n",
      "Polarity\n",
      "negative    3500\n",
      "neutral     3500\n",
      "positive    3500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define a function to perform random upsampling\n",
    "def upsample(df, target_column):\n",
    "    # Get the maximum count of samples in any class\n",
    "    max_count = df[target_column].value_counts().max()\n",
    "\n",
    "    # Separate each class and upsample the minority classes\n",
    "    upsampled_dfs = []\n",
    "    for label in df[target_column].unique():\n",
    "        # Get samples for the current label\n",
    "        df_label = df[df[target_column] == label]\n",
    "\n",
    "        # Upsample minority classes to match the majority class count\n",
    "        df_upsampled = resample(\n",
    "            df_label,\n",
    "            replace=True,            # Sample with replacement\n",
    "            n_samples=max_count,     # Match the number of samples in the majority class\n",
    "            random_state=42          # Set random seed for reproducibility\n",
    "        )\n",
    "        upsampled_dfs.append(df_upsampled)\n",
    "\n",
    "    # Combine the upsampled DataFrames\n",
    "    return pd.concat(upsampled_dfs)\n",
    "\n",
    "# Apply upsampling to 'Category' and 'Polarity'\n",
    "df_upsampled_category = upsample(df, 'Category')\n",
    "df_upsampled_polarity = upsample(df_upsampled_category, 'Polarity')\n",
    "\n",
    "# Shuffle the DataFrame to mix the resampled classes\n",
    "df_upsampled = df_upsampled_polarity.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Category distribution after upsampling:\")\n",
    "print(df_upsampled['Category'].value_counts())\n",
    "print(\"\\nPolarity distribution after upsampling:\")\n",
    "print(df_upsampled['Polarity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df_upsampled['Category_encoded'] = category_encoder.fit_transform(df_upsampled['Category'])\n",
    "df_upsampled['Polarity_encoded'] = polarity_encoder.fit_transform(df_upsampled['Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text using DistilBERT with padding and truncation\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "df_upsampled['tokens'] = df_upsampled['Text'].apply(lambda x: tokenize_function(x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_upsampled, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Dataset\n",
    "def create_tensor_dataset(df):\n",
    "    # Tokenize input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(list(df['Text']), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Convert labels to tensors\n",
    "    labels_category = tf.convert_to_tensor(df['Category_encoded'].values)\n",
    "    labels_polarity = tf.convert_to_tensor(df['Polarity_encoded'].values)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(((input_ids, attention_mask), (labels_category, labels_polarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = create_tensor_dataset(train_df)\n",
    "val_dataset = create_tensor_dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, GlobalAveragePooling1D\n",
    "from tensorflow.keras import Model\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Mini Conda\\envs\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and BERT model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize inputs\n",
    "def tokenize_inputs(texts, max_length=128):\n",
    "    inputs = tokenizer(\n",
    "        texts, return_tensors=\"tf\", padding=\"max_length\",\n",
    "        truncation=True, max_length=max_length\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN Layer with Residual Connections\n",
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation='relu', **kwargs):\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.dense = Dense(units, use_bias=False)\n",
    "\n",
    "    def call(self, features, adj_matrix):\n",
    "        # Normalize the adjacency matrix\n",
    "        adj_matrix = self.normalize_adjacency_matrix(adj_matrix)\n",
    "\n",
    "        # Perform graph convolution: H' = A*X*W\n",
    "        h = tf.matmul(adj_matrix, features)  # A*X\n",
    "        h = self.dense(h)  # Apply weight matrix W\n",
    "\n",
    "        # Apply the activation function\n",
    "        if self.activation:\n",
    "            h = tf.keras.activations.get(self.activation)(h)\n",
    "\n",
    "        # Add residual connection: Output = H + features (node-to-node residual)\n",
    "        h = h + features\n",
    "\n",
    "        return h\n",
    "\n",
    "    def normalize_adjacency_matrix(self, adj_matrix):\n",
    "        \"\"\" Normalize adjacency matrix by adding self-loops and applying symmetric normalization \"\"\"\n",
    "        # Add self-loops (identity matrix)\n",
    "        batch_size = tf.shape(adj_matrix)[0]\n",
    "        num_tokens = tf.shape(adj_matrix)[1]\n",
    "    \n",
    "        adj_matrix = adj_matrix + tf.eye(num_tokens, batch_shape=[batch_size], dtype=adj_matrix.dtype)\n",
    "    \n",
    "        # Symmetric normalization: D^(-1/2) * A * D^(-1/2)\n",
    "        degree_matrix = tf.reduce_sum(adj_matrix, axis=-1)\n",
    "        degree_matrix_inv_sqrt = tf.pow(degree_matrix + 1e-6, -0.5)\n",
    "        degree_matrix_inv_sqrt = tf.linalg.diag(degree_matrix_inv_sqrt)\n",
    "    \n",
    "        adj_matrix_normalized = tf.matmul(degree_matrix_inv_sqrt, adj_matrix)\n",
    "        adj_matrix_normalized = tf.matmul(adj_matrix_normalized, degree_matrix_inv_sqrt)\n",
    "    \n",
    "        return adj_matrix_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical GNN with Task-Specific Heads\n",
    "class HierarchicalGNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(HierarchicalGNN, self).__init__(**kwargs)\n",
    "        self.shared_gcn = GCNLayer(units)\n",
    "        self.task_specific_gcn = GCNLayer(units)\n",
    "        self.pooling = GlobalAveragePooling1D()\n",
    "\n",
    "    def call(self, features, adj_matrix):\n",
    "        shared_features = self.shared_gcn(features, adj_matrix)\n",
    "        task_specific_features = self.task_specific_gcn(shared_features, adj_matrix)\n",
    "        return task_specific_features\n",
    "    \n",
    "class GRUFusionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(GRUFusionLayer, self).__init__(**kwargs)\n",
    "        # Ensure GRU output matches adjusted features dimension (128)\n",
    "        self.gru = GRU(128, return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.gru(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiTask GNN Model with BERT and Residual Connections\n",
    "class MultiTaskGNNModel(Model):\n",
    "    def __init__(self, gnn_units, category_output_size, polarity_output_size, num_heads, **kwargs):\n",
    "        super(MultiTaskGNNModel, self).__init__(**kwargs)\n",
    "        self.distilbert = bert_model\n",
    "        self.gru_fusion = GRUFusionLayer(gnn_units)\n",
    "\n",
    "        # Multi-Head Attention Layer\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=64)\n",
    "\n",
    "        # Pooling layers to reduce the sequence dimension\n",
    "        self.category_pooling = GlobalAveragePooling1D()\n",
    "        self.polarity_pooling = GlobalAveragePooling1D()\n",
    "\n",
    "        # Separate task-specific Dense layers for final outputs\n",
    "        self.category_output_layer = Dense(category_output_size, activation='softmax', name='category_output')\n",
    "        self.polarity_output_layer = Dense(polarity_output_size, activation='softmax', name='polarity_output')\n",
    "\n",
    "        # Projection layer to reduce BERT embedding dimensionality to match attention mechanism\n",
    "        self.feature_adjustment_layer = Dense(128, activation='relu')\n",
    "\n",
    "        # Hierarchical GNN layers for each task with residual connection support\n",
    "        self.category_gnn = HierarchicalGNN(units=gnn_units)\n",
    "        self.polarity_gnn = HierarchicalGNN(units=gnn_units)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def call(self, input_ids, attention_mask):\n",
    "        # Pass through DistilBERT to get embeddings\n",
    "        bert_output = self.distilbert(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        # Project BERT embeddings to match the GNN units dimension\n",
    "        adjusted_features = self.feature_adjustment_layer(bert_output)\n",
    "\n",
    "        # Create dynamic adjacency matrix using multi-head attention\n",
    "        adj_matrix = self.adjacency_matrix(adjusted_features)\n",
    "\n",
    "        # GRU Fusion for Contextualized Embeddings\n",
    "        fused_features = self.gru_fusion(adjusted_features)\n",
    "\n",
    "        # Adding a residual connection between GRU output and original BERT embeddings\n",
    "        fused_features_with_residual = fused_features + adjusted_features\n",
    "\n",
    "        # Multi-Task GNN Heads with residual connections\n",
    "        category_features = self.category_gnn(fused_features_with_residual, adj_matrix)\n",
    "        category_features_with_residual = category_features + fused_features_with_residual\n",
    "\n",
    "        polarity_features = self.polarity_gnn(fused_features_with_residual, adj_matrix)\n",
    "        polarity_features_with_residual = polarity_features + fused_features_with_residual\n",
    "\n",
    "        # Pooling to collapse the sequence dimension\n",
    "        category_features_pooled = self.category_pooling(category_features_with_residual)\n",
    "        polarity_features_pooled = self.polarity_pooling(polarity_features_with_residual)\n",
    "\n",
    "        # Final task-specific outputs\n",
    "        category_output = self.category_output_layer(category_features_pooled)\n",
    "        polarity_output = self.polarity_output_layer(polarity_features_pooled)\n",
    "\n",
    "        return category_output, polarity_output\n",
    "\n",
    "    def adjacency_matrix(self, bert_embeddings):\n",
    "        # Multi-Head Attention to create adjacency matrix\n",
    "        attention_output = self.multi_head_attention(query=bert_embeddings, key=bert_embeddings, value=bert_embeddings)\n",
    "        adj_matrix = tf.nn.softmax(attention_output, axis=-1)\n",
    "\n",
    "        # identity matrix for self-loops\n",
    "        batch_size = tf.shape(adj_matrix)[0]\n",
    "        num_tokens = tf.shape(adj_matrix)[1]\n",
    "        identity_matrix = tf.eye(num_tokens, batch_shape=[batch_size], dtype=adj_matrix.dtype)\n",
    "        adj_matrix = adj_matrix + identity_matrix\n",
    "\n",
    "        return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " multi_task_gnn_model (MultiTas  ((None, 5),         66758920    ['input_ids[0][0]',              \n",
      " kGNNModel)                      (None, 3))                       'attention_mask[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,758,920\n",
      "Trainable params: 66,758,920\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "gnn_units = 128\n",
    "category_output_size = 5  \n",
    "polarity_output_size = 3 \n",
    "num_heads = 4\n",
    "\n",
    "multi_task_gnn_model = MultiTaskGNNModel(\n",
    "    gnn_units=gnn_units,\n",
    "    num_heads=num_heads,\n",
    "    category_output_size=category_output_size,\n",
    "    polarity_output_size=polarity_output_size\n",
    ")\n",
    "\n",
    "# Build model\n",
    "output_category, output_polarity = multi_task_gnn_model(input_ids, attention_mask)\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=[output_category, output_polarity])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Define the learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=1e-5,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Define the optimizer with the learning rate schedule\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy'],\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Metric to monitor\n",
    "    patience=8,                 # Number of epochs with no improvement\n",
    "    restore_best_weights=True,  # Restore model weights from the epoch with the best value of the monitored metric\n",
    "    mode='min',                 # In this case, we want to minimize the validation loss\n",
    "    verbose=1                   # Verbosity mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3432 - multi_task_gnn_model_loss: 1.3674 - multi_task_gnn_model_1_loss: 0.9757 - multi_task_gnn_model_accuracy: 0.4267 - multi_task_gnn_model_1_accuracy: 0.5048\n",
      "Epoch 1: val_loss improved from inf to 1.71488, saving model to best_model.h5\n",
      "473/473 [==============================] - 113s 213ms/step - loss: 2.3432 - multi_task_gnn_model_loss: 1.3674 - multi_task_gnn_model_1_loss: 0.9757 - multi_task_gnn_model_accuracy: 0.4267 - multi_task_gnn_model_1_accuracy: 0.5048 - val_loss: 1.7149 - val_multi_task_gnn_model_loss: 0.9654 - val_multi_task_gnn_model_1_loss: 0.7495 - val_multi_task_gnn_model_accuracy: 0.6690 - val_multi_task_gnn_model_1_accuracy: 0.6738\n",
      "Epoch 2/50\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.4484 - multi_task_gnn_model_loss: 0.8279 - multi_task_gnn_model_1_loss: 0.6206 - multi_task_gnn_model_accuracy: 0.6971 - multi_task_gnn_model_1_accuracy: 0.7427\n",
      "Epoch 2: val_loss improved from 1.71488 to 0.93825, saving model to best_model.h5\n",
      "473/473 [==============================] - 99s 208ms/step - loss: 1.4484 - multi_task_gnn_model_loss: 0.8279 - multi_task_gnn_model_1_loss: 0.6206 - multi_task_gnn_model_accuracy: 0.6971 - multi_task_gnn_model_1_accuracy: 0.7427 - val_loss: 0.9383 - val_multi_task_gnn_model_loss: 0.5648 - val_multi_task_gnn_model_1_loss: 0.3734 - val_multi_task_gnn_model_accuracy: 0.8262 - val_multi_task_gnn_model_1_accuracy: 0.8821\n",
      "Epoch 3/50\n",
      "473/473 [==============================] - ETA: 0s - loss: 0.9037 - multi_task_gnn_model_loss: 0.5402 - multi_task_gnn_model_1_loss: 0.3635 - multi_task_gnn_model_accuracy: 0.8180 - multi_task_gnn_model_1_accuracy: 0.8737\n",
      "Epoch 3: val_loss improved from 0.93825 to 0.66719, saving model to best_model.h5\n",
      "473/473 [==============================] - 100s 210ms/step - loss: 0.9037 - multi_task_gnn_model_loss: 0.5402 - multi_task_gnn_model_1_loss: 0.3635 - multi_task_gnn_model_accuracy: 0.8180 - multi_task_gnn_model_1_accuracy: 0.8737 - val_loss: 0.6672 - val_multi_task_gnn_model_loss: 0.4163 - val_multi_task_gnn_model_1_loss: 0.2508 - val_multi_task_gnn_model_accuracy: 0.8786 - val_multi_task_gnn_model_1_accuracy: 0.9250\n",
      "Epoch 4/50\n",
      " 84/473 [====>.........................] - ETA: 1:12 - loss: 0.6968 - multi_task_gnn_model_loss: 0.4424 - multi_task_gnn_model_1_loss: 0.2544 - multi_task_gnn_model_accuracy: 0.8430 - multi_task_gnn_model_1_accuracy: 0.9137"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set a higher number of epochs since EarlyStopping will likely stop earlier\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mf:\\Mini Conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset.batch(16), \n",
    "    epochs=50,  # Set a higher number of epochs since EarlyStopping will likely stop earlier\n",
    "    validation_data=val_dataset.batch(16),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
